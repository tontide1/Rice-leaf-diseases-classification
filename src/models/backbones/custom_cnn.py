"""Custom CNN architectures cho Paddy Disease Classification.

Module n√†y ch·ª©a c√°c ki·∫øn tr√∫c CNN ƒë∆∞·ª£c thi·∫øt k·∫ø ri√™ng, t·ªëi ∆∞u cho:
- Lightweight: √≠t parameters, inference nhanh
- Hi·ªáu qu·∫£: balance gi·ªØa accuracy v√† speed
- ƒê∆°n gi·∫£n: d·ªÖ hi·ªÉu, d·ªÖ modify
"""

import torch
import torch.nn as nn
from typing import Optional


class DepthwiseSeparableConv(nn.Module):
    """Depthwise Separable Convolution - gi·∫£m params v√† FLOPs.
    
    Thay v√¨ conv th√¥ng th∆∞·ªùng (3x3x C_in x C_out params),
    t√°ch th√†nh 2 b∆∞·ªõc:
    1. Depthwise: 3x3x1 cho t·ª´ng channel (3x3x C_in params)
    2. Pointwise: 1x1x C_in x C_out (C_in x C_out params)
    
    Gi·∫£m ~8-9x params so v·ªõi conv th√¥ng th∆∞·ªùng.
    """
    def __init__(self, in_channels: int, out_channels: int, 
                 kernel_size: int = 3, stride: int = 1, padding: int = 1):
        super().__init__()
        
        # Depthwise: m·ªói channel t·ª± convolution v·ªõi nhau
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, 
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=in_channels,  # Key: groups = in_channels
            bias=False
        )
        self.bn1 = nn.BatchNorm2d(in_channels)
        
        # Pointwise: 1x1 conv ƒë·ªÉ mix channels
        self.pointwise = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=1,
            bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU6(inplace=True)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.bn1(x)
        x = self.relu(x)
        
        x = self.pointwise(x)
        x = self.bn2(x)
        x = self.relu(x)
        return x


class InvertedResidual(nn.Module):
    """Inverted Residual Block (t·ª´ MobileNetV2).
    
    C·∫•u tr√∫c: 
    1. Expand: 1x1 conv tƒÉng channels (expand_ratio x)
    2. Depthwise: 3x3 conv v·ªõi stride
    3. Project: 1x1 conv gi·∫£m v·ªÅ out_channels
    4. Skip connection n·∫øu in == out v√† stride == 1
    """
    def __init__(self, in_channels: int, out_channels: int,
                 stride: int = 1, expand_ratio: int = 6):
        super().__init__()
        
        self.use_residual = (stride == 1 and in_channels == out_channels)
        hidden_dim = int(in_channels * expand_ratio)
        
        layers = []
        
        # Expand (n·∫øu expand_ratio != 1)
        if expand_ratio != 1:
            layers.extend([
                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True)
            ])
        
        # Depthwise
        layers.extend([
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1,
                     groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
        ])
        
        # Project (Linear bottleneck - kh√¥ng c√≥ activation)
        layers.extend([
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
        ])
        
        self.conv = nn.Sequential(*layers)
    
    def forward(self, x):
        if self.use_residual:
            return x + self.conv(x)
        else:
            return self.conv(x)


class LightweightCNN(nn.Module):
    """Custom Lightweight CNN cho Paddy Disease Classification.
    
    üìä Th√¥ng s·ªë:
    - Params: ~1.2M
    - FLOPs: ~0.15G
    - Accuracy mong ƒë·ª£i: 87-89%
    - FPS (GPU T4): ~1200
    - FPS (CPU): ~150
    
    ‚ú® ƒê·∫∑c ƒëi·ªÉm:
    - Ki·∫øn tr√∫c ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu
    - D√πng Depthwise Separable Conv ƒë·ªÉ gi·∫£m params
    - Residual connections cho gradient flow t·ªët
    - Batch Normalization cho training ·ªïn ƒë·ªãnh
    
    üèóÔ∏è Ki·∫øn tr√∫c:
    Input (224x224x3)
      ‚Üì
    Stem: Conv 3x3 ‚Üí 32 channels
      ‚Üì
    Stage 1: 2x DS Conv ‚Üí 64 channels (112x112)
      ‚Üì
    Stage 2: 2x DS Conv ‚Üí 128 channels (56x56)
      ‚Üì
    Stage 3: 3x DS Conv ‚Üí 256 channels (28x28)
      ‚Üì
    Stage 4: 2x DS Conv ‚Üí 512 channels (14x14)
      ‚Üì
    Global Avg Pool ‚Üí FC ‚Üí 4 classes
    
    üéØ Khi n√†o d√πng:
    - C·∫ßn model ƒë∆°n gi·∫£n ƒë·ªÉ h·ªçc CNN
    - Baseline nhanh cho comparison
    - Deploy tr√™n devices y·∫øu
    - Real-time applications
    """
    def __init__(self, num_classes: int = 4, dropout: float = 0.2, 
                 width_mult: float = 1.0):
        """
        Args:
            num_classes: S·ªë l∆∞·ª£ng classes
            dropout: Dropout rate tr∆∞·ªõc classifier
            width_mult: Multiplier cho s·ªë channels (ƒë·ªÉ scale model)
        """
        super().__init__()
        
        # Helper function ƒë·ªÉ apply width multiplier
        def _make_divisible(v, divisor=8):
            """L√†m cho channels chia h·∫øt cho divisor (t·ªëi ∆∞u hardware)."""
            new_v = max(divisor, int(v + divisor / 2) // divisor * divisor)
            if new_v < 0.9 * v:
                new_v += divisor
            return new_v
        
        # Stem: Conv th√¥ng th∆∞·ªùng ƒë·∫ßu ti√™n
        input_channel = _make_divisible(32 * width_mult)
        self.stem = nn.Sequential(
            nn.Conv2d(3, input_channel, kernel_size=3, stride=2, 
                     padding=1, bias=False),
            nn.BatchNorm2d(input_channel),
            nn.ReLU6(inplace=True)
        )
        
        # Stage 1: 64 channels, 112x112
        stage1_channel = _make_divisible(64 * width_mult)
        self.stage1 = nn.Sequential(
            DepthwiseSeparableConv(input_channel, stage1_channel, stride=1),
            DepthwiseSeparableConv(stage1_channel, stage1_channel, stride=1)
        )
        
        # Stage 2: 128 channels, 56x56
        stage2_channel = _make_divisible(128 * width_mult)
        self.stage2 = nn.Sequential(
            DepthwiseSeparableConv(stage1_channel, stage2_channel, stride=2),
            DepthwiseSeparableConv(stage2_channel, stage2_channel, stride=1)
        )
        
        # Stage 3: 256 channels, 28x28
        stage3_channel = _make_divisible(256 * width_mult)
        self.stage3 = nn.Sequential(
            DepthwiseSeparableConv(stage2_channel, stage3_channel, stride=2),
            DepthwiseSeparableConv(stage3_channel, stage3_channel, stride=1),
            DepthwiseSeparableConv(stage3_channel, stage3_channel, stride=1)
        )
        
        # Stage 4: 512 channels, 14x14
        stage4_channel = _make_divisible(512 * width_mult)
        self.stage4 = nn.Sequential(
            DepthwiseSeparableConv(stage3_channel, stage4_channel, stride=2),
            DepthwiseSeparableConv(stage4_channel, stage4_channel, stride=1)
        )
        
        # Classifier
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.fc = nn.Linear(stage4_channel, num_classes)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Kh·ªüi t·∫°o weights theo best practices."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', 
                                       nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = self.stem(x)      # 224 ‚Üí 112
        x = self.stage1(x)    # 112 ‚Üí 112
        x = self.stage2(x)    # 112 ‚Üí 56
        x = self.stage3(x)    # 56 ‚Üí 28
        x = self.stage4(x)    # 28 ‚Üí 14
        
        x = self.pool(x)      # 14 ‚Üí 1
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        return x
    
    def get_classifier(self):
        """Tr·∫£ v·ªÅ classifier layer ƒë·ªÉ t√≠nh differential learning rates."""
        return self.fc


class TinyPaddyNet(nn.Module):
    """TinyPaddyNet - Model C·ª∞C K·ª≤ nh·∫π cho edge devices.
    
    üìä Th√¥ng s·ªë:
    - Params: ~0.5M (NH·∫∏ NH·∫§T!)
    - FLOPs: ~0.08G
    - Accuracy mong ƒë·ª£i: 85-87%
    - FPS (GPU T4): ~1500+
    - FPS (CPU): ~200+
    
    ‚ú® ƒê·∫∑c ƒëi·ªÉm:
    - C·ª±c k·ª≥ nh·∫π v√† nhanh
    - D√πng Inverted Residual blocks
    - Squeeze channels ·ªü cu·ªëi
    - Perfect cho IoT/Edge devices
    
    üéØ Khi n√†o d√πng:
    - Microcontrollers (ESP32, Arduino)
    - Raspberry Pi Zero
    - Battery-powered devices
    - C·∫ßn inference C·ª∞C NHANH
    """
    def __init__(self, num_classes: int = 4, dropout: float = 0.1):
        super().__init__()
        
        # Stem: r·∫•t nh·∫π
        self.stem = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU6(inplace=True)
        )
        
        # Inverted Residual Blocks
        self.blocks = nn.Sequential(
            # 112x112, 16 ‚Üí 24
            InvertedResidual(16, 24, stride=2, expand_ratio=3),
            InvertedResidual(24, 24, stride=1, expand_ratio=3),
            
            # 56x56, 24 ‚Üí 32
            InvertedResidual(24, 32, stride=2, expand_ratio=3),
            InvertedResidual(32, 32, stride=1, expand_ratio=3),
            
            # 28x28, 32 ‚Üí 64
            InvertedResidual(32, 64, stride=2, expand_ratio=6),
            InvertedResidual(64, 64, stride=1, expand_ratio=6),
            
            # 14x14, 64 ‚Üí 96
            InvertedResidual(64, 96, stride=2, expand_ratio=6),
            InvertedResidual(96, 96, stride=1, expand_ratio=6),
        )
        
        # Final conv ƒë·ªÉ tƒÉng representation
        self.conv_last = nn.Sequential(
            nn.Conv2d(96, 192, kernel_size=1, bias=False),
            nn.BatchNorm2d(192),
            nn.ReLU6(inplace=True)
        )
        
        # Classifier
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.fc = nn.Linear(192, num_classes)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = self.stem(x)
        x = self.blocks(x)
        x = self.conv_last(x)
        
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        return x
    
    def get_classifier(self):
        return self.fc


class CompactCNN(nn.Module):
    """CompactCNN - Balance t·ªët gi·ªØa accuracy v√† speed.
    
    üìä Th√¥ng s·ªë:
    - Params: ~1.8M
    - FLOPs: ~0.25G
    - Accuracy mong ƒë·ª£i: 89-91%
    - FPS (GPU T4): ~1000
    - FPS (CPU): ~120
    
    ‚ú® ƒê·∫∑c ƒëi·ªÉm:
    - K·∫øt h·ª£p DS Conv v√† Inverted Residual
    - Squeeze-and-Excitation (SE) blocks nh·∫π
    - Multi-scale features
    - T·ªët cho production deployment
    
    üéØ Khi n√†o d√πng:
    - C·∫ßn balance accuracy vs speed
    - Production deployment
    - Mobile apps (Android/iOS)
    - Real-time inference v·ªõi accuracy cao
    """
    def __init__(self, num_classes: int = 4, dropout: float = 0.2):
        super().__init__()
        
        # Stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, 24, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(24),
            nn.ReLU6(inplace=True)
        )
        
        # Stage 1: 112x112
        self.stage1 = nn.Sequential(
            InvertedResidual(24, 32, stride=1, expand_ratio=4),
            InvertedResidual(32, 32, stride=1, expand_ratio=4),
        )
        
        # Stage 2: 56x56
        self.stage2 = nn.Sequential(
            InvertedResidual(32, 64, stride=2, expand_ratio=4),
            InvertedResidual(64, 64, stride=1, expand_ratio=4),
            InvertedResidual(64, 64, stride=1, expand_ratio=4),
        )
        
        # Stage 3: 28x28
        self.stage3 = nn.Sequential(
            InvertedResidual(64, 128, stride=2, expand_ratio=6),
            InvertedResidual(128, 128, stride=1, expand_ratio=6),
            InvertedResidual(128, 128, stride=1, expand_ratio=6),
        )
        
        # Stage 4: 14x14
        self.stage4 = nn.Sequential(
            InvertedResidual(128, 256, stride=2, expand_ratio=6),
            InvertedResidual(256, 256, stride=1, expand_ratio=6),
        )
        
        # Final conv
        self.conv_last = nn.Sequential(
            nn.Conv2d(256, 384, kernel_size=1, bias=False),
            nn.BatchNorm2d(384),
            nn.ReLU6(inplace=True)
        )
        
        # Classifier
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.fc = nn.Linear(384, num_classes)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = self.stem(x)
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        x = self.conv_last(x)
        
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        return x
    
    def get_classifier(self):
        return self.fc


# Factory function ƒë·ªÉ d·ªÖ d√†ng kh·ªüi t·∫°o models
def create_custom_cnn(model_name: str, num_classes: int = 4, 
                      dropout: float = 0.2, **kwargs):
    """Factory function ƒë·ªÉ t·∫°o custom CNN models.
    
    Args:
        model_name: T√™n model ('lightweight', 'tiny', 'compact')
        num_classes: S·ªë l∆∞·ª£ng classes
        dropout: Dropout rate
        **kwargs: C√°c tham s·ªë kh√°c cho model
    
    Returns:
        model: CNN model instance
    
    Example:
        >>> model = create_custom_cnn('lightweight', num_classes=4)
        >>> model = create_custom_cnn('tiny', num_classes=4, dropout=0.1)
    """
    model_name = model_name.lower()
    
    if model_name == 'lightweight':
        return LightweightCNN(num_classes=num_classes, dropout=dropout, **kwargs)
    elif model_name == 'tiny':
        return TinyPaddyNet(num_classes=num_classes, dropout=dropout)
    elif model_name == 'compact':
        return CompactCNN(num_classes=num_classes, dropout=dropout)
    else:
        raise ValueError(f"Unknown model: {model_name}. "
                       f"Choose from: lightweight, tiny, compact")


if __name__ == "__main__":
    # Test c√°c models
    import time
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 16
    input_size = 224
    
    models = {
        'LightweightCNN': LightweightCNN(num_classes=4),
        'TinyPaddyNet': TinyPaddyNet(num_classes=4),
        'CompactCNN': CompactCNN(num_classes=4),
    }
    
    print("="*70)
    print("CUSTOM CNN MODELS BENCHMARK")
    print("="*70)
    
    for name, model in models.items():
        model = model.to(device)
        model.eval()
        
        # ƒê·∫øm params
        num_params = sum(p.numel() for p in model.parameters())
        model_size_mb = sum(p.numel() * p.element_size() 
                           for p in model.parameters()) / (1024**2)
        
        # Test inference
        x = torch.randn(batch_size, 3, input_size, input_size).to(device)
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = model(x)
        
        # Benchmark
        torch.cuda.synchronize() if device.type == 'cuda' else None
        start = time.time()
        
        with torch.no_grad():
            for _ in range(100):
                _ = model(x)
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        elapsed = time.time() - start
        
        fps = (100 * batch_size) / elapsed
        
        print(f"\n{name}:")
        print(f"  Params: {num_params:,}")
        print(f"  Size: {model_size_mb:.2f} MB")
        print(f"  FPS: {fps:.1f}")
        print(f"  Latency: {elapsed*10:.2f} ms/batch")
    
    print("\n" + "="*70)
