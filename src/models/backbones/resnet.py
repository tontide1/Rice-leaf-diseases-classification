import torch
import torch.nn as nn
import timm

from ..attention import (
    BoTNetBlock, 
    BoTNetBlockLinear,
    CABlock,
    ECABlock,
    CoordinateAttention,
    ECAttention
)

# ============================================================================
# KIáº¾N TRÃšC Gá»C
# ============================================================================

class ResNet18_BoT(nn.Module):
    """ResNet18 vá»›i BoT (Bottleneck Transformer) - Accuracy: 93-95%"""
    def __init__(self, num_classes, heads, pretrained=True, dropout: float = 0.0):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        self.bot_block = BoTNetBlock(512, 512, heads)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        x = self.bot_block(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


class ResNet18_BoTLinear(nn.Module):
    """ResNet18 vá»›i BoTLinear (Linear Attention) - Accuracy: 92-94%, Faster"""
    def __init__(self, num_classes, heads, pretrained=True, dropout: float = 0.0, bottleneck_dim=None):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        self.bot_block = BoTNetBlockLinear(512, 512, heads, bottleneck_dim=bottleneck_dim)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        x = self.bot_block(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


# ============================================================================
# KIáº¾N TRÃšC Má»šI Äá»€ XUáº¤T
# ============================================================================

class ResNet18_CA(nn.Module):
    """ResNet18 vá»›i Coordinate Attention block.
    
    ðŸ“Š ThÃ´ng sá»‘:
    - Accuracy mong Ä‘á»£i: 92-94%
    - Tá»‘c Ä‘á»™: Nhanh hÆ¡n BoT ~15%
    - Params: Ãt hÆ¡n BoT ~20%
    
    âœ¨ Æ¯u Ä‘iá»ƒm:
    - Táº­p trung vÃ o spatial location (height + width)
    - Nháº¹ vÃ  nhanh
    - Tá»‘t cho localize vá»‹ trÃ­ bá»‡nh trÃªn lÃ¡
    
    ðŸŽ¯ Khi nÃ o dÃ¹ng:
    - Cáº§n balance giá»¯a accuracy vÃ  speed
    - Bá»‡nh cÃ³ vá»‹ trÃ­ Ä‘áº·c trÆ°ng trÃªn lÃ¡
    - Deploy trÃªn edge devices
    """
    def __init__(self, num_classes, reduction=32, pretrained=True, dropout: float = 0.0):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        # Coordinate Attention: encode spatial information
        self.ca_block = CABlock(512, 512, reduction=reduction)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        x = self.ca_block(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


class ResNet18_ECA(nn.Module):
    """ResNet18 vá»›i Efficient Channel Attention.
    
    ðŸ“Š ThÃ´ng sá»‘:
    - Accuracy mong Ä‘á»£i: 91-93%
    - Tá»‘c Ä‘á»™: Nhanh nháº¥t (~30% faster than BoT)
    - Params: Chá»‰ thÃªm ~0.01M (minimal overhead)
    
    âœ¨ Æ¯u Ä‘iá»ƒm:
    - Cá»±c ká»³ nháº¹ vÃ  nhanh
    - 1D convolution cho channel attention
    - KhÃ´ng cÃ³ dimensionality reduction
    - Perfect cho production deployment
    
    ðŸŽ¯ Khi nÃ o dÃ¹ng:
    - Æ¯u tiÃªn tá»‘c Ä‘á»™ inference
    - Mobile/Edge deployment
    - Real-time applications
    - Limited computational resources
    """
    def __init__(self, num_classes, k_size=3, pretrained=True, dropout: float = 0.0):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        # ECA: Efficient Channel Attention (very lightweight)
        self.eca_block = ECABlock(512, 512, k_size=k_size)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        x = self.eca_block(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


class ResNet18_Hybrid(nn.Module):
    """ResNet18 Hybrid: BoTLinear + Coordinate Attention.
    
    ðŸ“Š ThÃ´ng sá»‘:
    - Accuracy mong Ä‘á»£i: 94-96% (CAO NHáº¤T!)
    - Tá»‘c Ä‘á»™: Trung bÃ¬nh (cháº­m hÆ¡n BoT ~10%)
    - Params: Cao nháº¥t (+30% so vá»›i base)
    
    âœ¨ Æ¯u Ä‘iá»ƒm:
    - Káº¿t há»£p Ä‘iá»ƒm máº¡nh cá»§a cáº£ BoTLinear vÃ  CA
    - BoTLinear: Global context via linear attention
    - CA: Spatial localization via coordinate attention
    - Há»c Ä‘Æ°á»£c cáº£ global + local features
    
    ðŸŽ¯ Khi nÃ o dÃ¹ng:
    - Æ¯u tiÃªn accuracy cao nháº¥t
    - Äá»§ computational resources
    - BÃ i toÃ¡n khÃ³, cáº§n mÃ´ hÃ¬nh máº¡nh
    - Research/Competition
    """
    def __init__(self, num_classes, heads=4, reduction=32, pretrained=True, dropout: float = 0.0, bottleneck_dim=None):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        # Hybrid: BoTLinear + CA
        self.bot_block = BoTNetBlockLinear(512, 512, heads, bottleneck_dim=bottleneck_dim)
        self.ca_block = CABlock(512, 512, reduction=reduction)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        
        # Sequential: Spatial first (more stable), then global
        x = self.ca_block(x)     # Coordinate Attention first
        x = self.bot_block(x)    # BoTLinear second
        
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


class ResNet18_MultiScale(nn.Module):
    """ResNet18 vá»›i Multi-Scale Attention.
    
    ðŸ“Š ThÃ´ng sá»‘:
    - Accuracy mong Ä‘á»£i: 93-95%
    - Tá»‘c Ä‘á»™: Cháº­m hÆ¡n base ~20%
    - Params: Cao (+40% so vá»›i base)
    
    âœ¨ Æ¯u Ä‘iá»ƒm:
    - Attention á»Ÿ nhiá»u scales (layer3: 256ch, layer4: 512ch)
    - Rich multi-scale feature extraction
    - Tá»‘t cho diseases cÃ³ nhiá»u kÃ­ch thÆ°á»›c
    - Robust vá»›i different image resolutions
    
    ðŸŽ¯ Khi nÃ o dÃ¹ng:
    - Bá»‡nh cÃ³ nhiá»u scales khÃ¡c nhau
    - Dataset cÃ³ varied image sizes
    - Cáº§n rich feature representations
    - Äá»§ GPU memory
    """
    def __init__(self, num_classes, heads=4, pretrained=True, dropout: float = 0.0):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        # Multi-scale: attention at different layers
        self.attention_layer3 = ECABlock(256, 256, k_size=3)  # Smaller scale
        self.attention_layer4 = BoTNetBlockLinear(512, 512, heads)  # Larger scale

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        
        # Attention at layer3 (256 channels, smaller features)
        x = self.layer3(x)
        x = self.attention_layer3(x)
        
        # Attention at layer4 (512 channels, larger features)
        x = self.layer4(x)
        x = self.attention_layer4(x)
        
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc


class ResNet18_Lightweight(nn.Module):
    """ResNet18 Lightweight: Tá»‘i Æ°u cho inference speed.
    
    ðŸ“Š ThÃ´ng sá»‘:
    - Accuracy mong Ä‘á»£i: 90-92%
    - Tá»‘c Ä‘á»™: Nhanh nháº¥t (~35% faster than BoT)
    - Params: Minimal (+0.01M only)
    - Latency: <10ms on GPU
    
    âœ¨ Æ¯u Ä‘iá»ƒm:
    - Chá»‰ sá»­ dá»¥ng lightweight ECA
    - Minimal computational overhead
    - Perfect cho production deployment
    - Low power consumption
    
    ðŸŽ¯ Khi nÃ o dÃ¹ng:
    - Production deployment
    - Real-time inference requirements
    - Mobile/Edge devices
    - Battery-powered devices
    - High throughput needed
    """
    def __init__(self, num_classes, k_size=3, pretrained=True, dropout: float = 0.0):
        super().__init__()
        backbone = timm.create_model(
            "resnet18",
            pretrained=pretrained,
            num_classes=0
        )

        self.stem = nn.Sequential(backbone.conv1, backbone.bn1, backbone.act1, backbone.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = backbone.layer1, backbone.layer2, backbone.layer3, backbone.layer4

        # Only lightweight ECA attention
        self.eca = ECAttention(k_size=k_size)

        self.pool = backbone.global_pool
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))
        x = self.eca(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)
    
    def get_classifier(self):
        return self.fc
