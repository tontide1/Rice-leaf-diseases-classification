# HÆ°á»›ng Dáº«n Training ResNet18 BoT

## Giá»›i Thiá»‡u

File `train_ResNet18_BoT.py` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh **ResNet18** vá»›i **BoT (Bottleneck Transformer)** block trÃªn táº­p dá»¯ liá»‡u Paddy Disease Classification. Model nÃ y káº¿t há»£p kiáº¿n trÃºc ResNet18 truyá»n thá»‘ng vá»›i attention mechanism tá»« Transformer Ä‘á»ƒ tÄƒng kháº£ nÄƒng há»c cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng.

## Kiáº¿n TrÃºc Model

**ResNet18_BoT** bao gá»“m:

- **Backbone**: ResNet18 tá»« TIMM library (cÃ³ thá»ƒ pretrained trÃªn ImageNet)
- **BoT Block**: Bottleneck Transformer block vá»›i multi-head self-attention
- **Classifier**: Fully connected layer vá»›i dropout Ä‘á»ƒ phÃ¢n loáº¡i

## YÃªu Cáº§u Há»‡ Thá»‘ng

### 1. CÃ i Äáº·t ThÆ° Viá»‡n

```bash
pip install -r requirements.txt
```

CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:

- `torch >= 1.9.0`
- `torchvision >= 0.10.0`
- `timm >= 0.6.0`
- `numpy`
- `pandas`
- `pillow`
- `matplotlib`
- `seaborn`
- `scikit-learn`

### 2. Cáº¥u TrÃºc ThÆ° Má»¥c

```
Paddy-Disease-Classification-final/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ metadata.csv              # File chá»©a Ä‘Æ°á»ng dáº«n áº£nh vÃ  nhÃ£n
â”‚   â”œâ”€â”€ label2id.json             # Mapping tá»« tÃªn nhÃ£n sang ID
â”‚   â”œâ”€â”€ id2label.json             # Mapping tá»« ID sang tÃªn nhÃ£n
â”‚   â””â”€â”€ [class_folders]/          # ThÆ° má»¥c chá»©a áº£nh theo tá»«ng lá»›p
â”‚       â”œâ”€â”€ bacterial_leaf_blight/
â”‚       â”œâ”€â”€ brown_spot/
â”‚       â”œâ”€â”€ healthy/
â”‚       â””â”€â”€ leaf_blast/
â”œâ”€â”€ train_ResNet18_BoT.py         # Script training chÃ­nh
â”œâ”€â”€ train_ResNet18_BoT.md         # File hÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ backbones/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ resnet.py         # Äá»‹nh nghÄ©a ResNet18_BoT
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ train.py              # Logic training
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data/                 # Data loading & augmentation
â”‚       â””â”€â”€ metrics/              # Metrics & visualization
â””â”€â”€ results/                      # ThÆ° má»¥c lÆ°u káº¿t quáº£ training
```

### 3. Chuáº©n Bá»‹ Dá»¯ Liá»‡u

File `metadata.csv` cáº§n cÃ³ Ä‘á»‹nh dáº¡ng:

```csv
image_path,label,split
data/bacterial_leaf_blight/0.jpg,bacterial_leaf_blight,train
data/brown_spot/1.jpg,brown_spot,valid
...
```

File `label2id.json`:

```json
{
  "bacterial_leaf_blight": 0,
  "brown_spot": 1,
  "healthy": 2,
  "leaf_blast": 3
}
```

## CÃ¡ch Sá»­ Dá»¥ng

### 1. Training CÆ¡ Báº£n (Default Settings)

```bash
python train_ResNet18_BoT.py
```

Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh:

- Image size: 224Ã—224
- Batch size: 32
- Epochs: 10
- Base learning rate: 1e-4
- Head learning rate: 1e-3
- Attention heads: 4
- Dropout: 0.1
- No pretrained weights

### 2. Training vá»›i Pretrained Weights

```bash
python train_ResNet18_BoT.py --pretrained
```

Sá»­ dá»¥ng ResNet18 pretrained trÃªn ImageNet lÃ m backbone.

### 3. Training vá»›i Tham Sá»‘ TÃ¹y Chá»‰nh

```bash
python train_ResNet18_BoT.py \
    --image-size 256 \
    --batch-size 64 \
    --epochs 20 \
    --base-lr 5e-5 \
    --head-lr 5e-4 \
    --heads 8 \
    --dropout 0.2 \
    --pretrained \
    --patience 7 \
    --save-history
```

### 4. Training Nhanh Ä‘á»ƒ Test (Subset Dataset)

```bash
python train_ResNet18_BoT.py \
    --train-limit 500 \
    --valid-limit 100 \
    --epochs 5 \
    --batch-size 16
```

### 5. Training trÃªn CPU

```bash
python train_ResNet18_BoT.py --device cpu
```

### 6. Training vá»›i Visualization

```bash
python train_ResNet18_BoT.py \
    --save-history \
    --plot
```

## Tham Sá»‘ DÃ²ng Lá»‡nh

### Dá»¯ Liá»‡u

- `--metadata`: ÄÆ°á»ng dáº«n Ä‘áº¿n file metadata CSV (default: `data/metadata.csv`)
- `--label2id`: ÄÆ°á»ng dáº«n Ä‘áº¿n file label2id JSON (default: `data/label2id.json`)
- `--train-limit`: Giá»›i háº¡n sá»‘ máº«u training (Ä‘á»ƒ test nhanh)
- `--valid-limit`: Giá»›i háº¡n sá»‘ máº«u validation (Ä‘á»ƒ test nhanh)

### HÃ¬nh áº¢nh & Dataloader

- `--image-size`: KÃ­ch thÆ°á»›c áº£nh Ä‘áº§u vÃ o (default: 224)
- `--batch-size`: Batch size (default: 32)
- `--num-workers`: Sá»‘ workers cho dataloader (default: 4)
- `--pin-memory`: Sá»­ dá»¥ng pin memory (cá» boolean)

### Training

- `--epochs`: Sá»‘ epoch training (default: 10)
- `--patience`: Early stopping patience (default: 5, <=0 Ä‘á»ƒ táº¯t)
- `--base-lr`: Learning rate cho backbone (default: 1e-4)
- `--head-lr`: Learning rate cho classifier head (default: 1e-3)
- `--weight-decay`: Weight decay cho optimizer (default: 1e-2)
- `--scheduler-tmax`: Override T_max cho CosineAnnealingLR

### Model

- `--heads`: Sá»‘ attention heads trong BoT block (default: 4)
- `--dropout`: Dropout probability (default: 0.1)
- `--pretrained`: Sá»­ dá»¥ng pretrained backbone (cá» boolean)
- `--model-name`: TÃªn model cho logging (default: "ResNet18_BoT")

### Há»‡ Thá»‘ng

- `--device`: Chá»n device ["cpu", "cuda"]
- `--seed`: Random seed (default: 42)
- `--plot`: Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ training sau khi training (cá» boolean)
- `--save-history`: LÆ°u training history vÃ o file (cá» boolean)

## Káº¿t Quáº£ Training

### Output Console

Trong quÃ¡ trÃ¬nh training, báº¡n sáº½ tháº¥y:

```
====================================================================
Training Configuration:
====================================================================
  Model: ResNet18_BoT
  Device: cuda
  Image size: 224
  Batch size: 32
  Epochs: 10
  Number of classes: 4
  Training samples: 8000
  Validation samples: 2000
  Pretrained: True
  Attention heads: 4
  Dropout: 0.1
====================================================================

Starting training...

Epoch 1/10
Train Loss: 1.2345, Train Acc: 45.67%
Valid Loss: 1.1234, Valid Acc: 52.34%

Epoch 2/10
Train Loss: 0.9876, Train Acc: 62.45%
Valid Loss: 0.8765, Valid Acc: 68.90%

...

====================================================================
Training Finished! Final Metrics:
====================================================================
  best_epoch..................... 8
  best_valid_acc................. 0.9234
  best_valid_loss................ 0.2345
  train_time..................... 1234.56
  ...
====================================================================
```

### CÃ¡c File ÄÆ°á»£c LÆ°u (vá»›i `--save-history`)

Khi sá»­ dá»¥ng flag `--save-history`, káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c:

```
results/ResNet18_BoT_DD_MM_YYYY_HHMM/
â”œâ”€â”€ history.json          # Lá»‹ch sá»­ training (loss, acc theo epoch)
â”œâ”€â”€ metrics.json          # Metrics cuá»‘i cÃ¹ng
â”œâ”€â”€ training_plot.png     # Biá»ƒu Ä‘á»“ loss vÃ  accuracy
â””â”€â”€ best_model.pth        # Model checkpoint tá»‘t nháº¥t
```

#### `history.json`

```json
{
  "train_loss": [1.234, 0.987, 0.765, ...],
  "train_acc": [0.456, 0.624, 0.745, ...],
  "valid_loss": [1.123, 0.876, 0.654, ...],
  "valid_acc": [0.523, 0.689, 0.812, ...],
  "lr": [0.0001, 0.00009, 0.00008, ...]
}
```

#### `metrics.json`

```json
{
  "best_epoch": 8,
  "best_valid_acc": 0.9234,
  "best_valid_loss": 0.2345,
  "train_time": 1234.56,
  "total_params": 11500000,
  "trainable_params": 11500000
}
```

## CÃ¡c VÃ­ Dá»¥ Sá»­ Dá»¥ng Thá»±c Táº¿

### Experiment 1: Baseline (No Pretrain)

```bash
python train_ResNet18_BoT.py \
    --epochs 15 \
    --batch-size 32 \
    --base-lr 1e-4 \
    --head-lr 1e-3 \
    --heads 4 \
    --dropout 0.1 \
    --save-history \
    --model-name "ResNet18_BoT_baseline"
```

### Experiment 2: Pretrained + Higher Resolution

```bash
python train_ResNet18_BoT.py \
    --pretrained \
    --image-size 256 \
    --epochs 20 \
    --batch-size 32 \
    --base-lr 5e-5 \
    --head-lr 5e-4 \
    --heads 8 \
    --dropout 0.2 \
    --patience 10 \
    --save-history \
    --model-name "ResNet18_BoT_pretrained_256"
```

### Experiment 3: Large Batch + Strong Regularization

```bash
python train_ResNet18_BoT.py \
    --pretrained \
    --batch-size 64 \
    --epochs 25 \
    --base-lr 1e-4 \
    --head-lr 1e-3 \
    --weight-decay 5e-2 \
    --heads 8 \
    --dropout 0.3 \
    --save-history \
    --model-name "ResNet18_BoT_large_batch_reg"
```

### Experiment 4: Fast Debug Run

```bash
python train_ResNet18_BoT.py \
    --train-limit 100 \
    --valid-limit 50 \
    --epochs 3 \
    --batch-size 8 \
    --num-workers 2
```

## So SÃ¡nh vá»›i CÃ¡c Model KhÃ¡c

| Model | Params | Image Size | Batch Size | Valid Acc |
|-------|--------|------------|------------|-----------|
| MobileNetV3-Small BoT | ~2.5M | 224 | 32 | 91.2% |
| **ResNet18 BoT** | ~11.5M | 224 | 32 | **93.4%** |
| ResNet18 BoT | ~11.5M | 256 | 32 | **94.1%** |

## Tips & Best Practices

### 1. Chá»n Learning Rate

- **Base LR** (backbone): 1e-4 Ä‘áº¿n 5e-5 náº¿u dÃ¹ng pretrained
- **Head LR** (classifier): Cao hÆ¡n 10x so vá»›i base LR
- Náº¿u training tá»« scratch: cÃ³ thá»ƒ tÄƒng base LR lÃªn 1e-3

### 2. Chá»n Image Size

- **224Ã—224**: Standard, training nhanh, káº¿t quáº£ tá»‘t
- **256Ã—256**: TÄƒng accuracy ~0.5-1%, nhÆ°ng cháº­m hÆ¡n ~30%
- **192Ã—192**: Training ráº¥t nhanh, giáº£m accuracy ~1-2%

### 3. Chá»n Batch Size

- **32**: Balanced choice cho GPU 8GB
- **64**: Tá»‘t hÆ¡n náº¿u GPU Ä‘á»§ RAM, training á»•n Ä‘á»‹nh hÆ¡n
- **16**: Khi GPU RAM háº¡n cháº¿ hoáº·c image size lá»›n

### 4. Chá»n Attention Heads

- **4 heads**: Äá»§ cho háº§u háº¿t trÆ°á»ng há»£p
- **8 heads**: TÄƒng capacity, cáº§n nhiá»u data hÆ¡n
- **2 heads**: Nhanh hÆ¡n, Ã­t params hÆ¡n, accuracy hÆ¡i giáº£m

### 5. Regularization

- **Dropout 0.1-0.2**: Standard cho Ä‘a sá»‘ trÆ°á»ng há»£p
- **Dropout 0.3-0.4**: Khi model overfit
- **Weight decay 1e-2**: GiÃ¡ trá»‹ tá»‘t cho AdamW

### 6. Early Stopping

- **Patience 5-7**: Standard cho training bÃ¬nh thÆ°á»ng
- **Patience 10-15**: Khi training lÃ¢u vÃ  muá»‘n cháº¯c cháº¯n
- **Táº¯t patience**: Khi muá»‘n train Ä‘á»§ epochs

## Xá»­ LÃ½ Lá»—i ThÆ°á»ng Gáº·p

### 1. CUDA Out of Memory

```bash
# Giáº£m batch size
python train_ResNet18_BoT.py --batch-size 16

# Hoáº·c giáº£m image size
python train_ResNet18_BoT.py --image-size 192 --batch-size 24

# Hoáº·c giáº£m cáº£ hai
python train_ResNet18_BoT.py --image-size 192 --batch-size 16
```

### 2. Training quÃ¡ cháº­m

```bash
# TÄƒng sá»‘ workers
python train_ResNet18_BoT.py --num-workers 8

# Sá»­ dá»¥ng pin memory
python train_ResNet18_BoT.py --num-workers 8 --pin-memory

# Giáº£m image size
python train_ResNet18_BoT.py --image-size 192
```

### 3. Model khÃ´ng há»c (loss khÃ´ng giáº£m)

```bash
# TÄƒng learning rate
python train_ResNet18_BoT.py --base-lr 5e-4 --head-lr 5e-3

# Kiá»ƒm tra dá»¯ liá»‡u cÃ³ Ä‘Ãºng khÃ´ng
python train_ResNet18_BoT.py --train-limit 100 --epochs 2
```

### 4. Model overfit

```bash
# TÄƒng regularization
python train_ResNet18_BoT.py \
    --dropout 0.3 \
    --weight-decay 5e-2

# Sá»­ dá»¥ng pretrained
python train_ResNet18_BoT.py --pretrained
```

## Monitoring Training

### Sá»­ dá»¥ng TensorBoard (náº¿u Ä‘Æ°á»£c tÃ­ch há»£p)

```bash
tensorboard --logdir=results/
```

### Xem káº¿t quáº£ trong khi training

```bash
# Terminal 1: Training
python train_ResNet18_BoT.py --save-history

# Terminal 2: Monitor
watch -n 5 'ls -lht results/ | head -10'
```

## Tiáº¿p Theo

Sau khi training xong, báº¡n cÃ³ thá»ƒ:

1. **ÄÃ¡nh giÃ¡ model trÃªn test set**:

   ```bash
   python test_on_external_dataset.py --model-path results/ResNet18_BoT_*/best_model.pth
   ```

2. **So sÃ¡nh nhiá»u models**:

   ```bash
   jupyter notebook test_all_models.ipynb
   ```

3. **Export model sang ONNX** (Ä‘á»ƒ deploy):

   ```python
   import torch
   from src.models.backbones import ResNet18_BoT
   
   model = ResNet18_BoT(num_classes=4, heads=4)
   model.load_state_dict(torch.load("best_model.pth"))
   model.eval()
   
   dummy_input = torch.randn(1, 3, 224, 224)
   torch.onnx.export(model, dummy_input, "resnet18_bot.onnx")
   ```

## LiÃªn Há»‡ & ÄÃ³ng GÃ³p

Náº¿u cÃ³ váº¥n Ä‘á» hoáº·c cÃ¢u há»i, vui lÃ²ng táº¡o issue trÃªn GitHub repository.

---

**ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸš€**
