"""Train ResNet18 BoTLinear backbone on Paddy dataset."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Optional
import json
from datetime import datetime

import torch
from torch import nn
from torch.cuda.amp import GradScaler
from torch.utils.data import DataLoader, Subset

# ROOT_DIR tr·ªè ƒë·∫øn th∆∞ m·ª•c g·ªëc c·ªßa project (th∆∞ m·ª•c hi·ªán t·∫°i)
ROOT_DIR = Path(__file__).resolve().parent
SRC_DIR = ROOT_DIR / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))
# Th√™m ROOT_DIR v√†o sys.path ƒë·ªÉ import t·ª´ src
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from src.models.backbones import ResNet18_BoTLinear  # noqa: E402
from src.training import get_param_groups, train_model  # noqa: E402
from src.utils.data import (  # noqa: E402
    TransformConfig,
    build_datasets,
)
from src.utils.metrics.plot import plot_history  # noqa: E402

try:
    import matplotlib
    matplotlib.use('Agg')  # Backend kh√¥ng t∆∞∆°ng t√°c ƒë·ªÉ l∆∞u bi·ªÉu ƒë·ªì
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


def parse_args() -> argparse.Namespace:
    """Parse c√°c tham s·ªë d√≤ng l·ªánh."""
    parser = argparse.ArgumentParser(description="Train ResNet18 BoTLinear tr√™n Paddy dataset")
    
    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
    parser.add_argument("--metadata", type=Path, default=Path(__file__).parent / "data/metadata.csv",
                        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file metadata CSV")
    parser.add_argument("--label2id", type=Path, default=Path(__file__).parent / "data/label2id.json",
                        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file label2id JSON")
    
    # Tham s·ªë h√¨nh ·∫£nh v√† training
    parser.add_argument("--image-size", type=int, default=224, help="K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size cho training")
    parser.add_argument("--epochs", type=int, default=10, help="S·ªë epoch training")
    parser.add_argument("--patience", type=int, default=5, help="Early stopping patience (<=0 ƒë·ªÉ t·∫Øt)")
    
    # Tham s·ªë optimizer
    parser.add_argument("--base-lr", type=float, default=1e-4, help="Learning rate cho backbone")
    parser.add_argument("--head-lr", type=float, default=1e-3, help="Learning rate cho classifier head")
    parser.add_argument("--weight-decay", type=float, default=1e-2, help="Weight decay cho optimizer")
    
    # Tham s·ªë model
    parser.add_argument("--heads", type=int, default=4, help="S·ªë attention heads trong BoTLinear block")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout probability tr∆∞·ªõc classifier")
    parser.add_argument("--pretrained", action="store_true", default=False,
                        help="S·ª≠ d·ª•ng backbone pretrained t·ª´ ImageNet")
    
    # Tham s·ªë dataloader
    parser.add_argument("--num-workers", type=int, default=4, help="S·ªë workers cho dataloader")
    parser.add_argument("--pin-memory", action="store_true", default=False,
                        help="Pin memory trong dataloaders")
    
    # Gi·ªõi h·∫°n d·ªØ li·ªáu (ƒë·ªÉ test nhanh)
    parser.add_argument("--train-limit", type=int, default=None,
                        help="Gi·ªõi h·∫°n s·ªë m·∫´u training (ƒë·ªÉ ch·∫°y nhanh)")
    parser.add_argument("--valid-limit", type=int, default=None,
                        help="Gi·ªõi h·∫°n s·ªë m·∫´u validation")
    
    # Tham s·ªë kh√°c
    parser.add_argument("--model-name", type=str, default="ResNet18_BoTLinear",
                        help="T√™n model cho logging/checkpoint")
    parser.add_argument("--scheduler-tmax", type=int, default=None,
                        help="Override T_max cho CosineAnnealingLR")
    parser.add_argument("--device", type=str, default=None, choices=["cpu", "cuda"],
                        help="Ch·ªçn device c·ª• th·ªÉ")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--plot", action="store_true", default=False,
                        help="V·∫Ω bi·ªÉu ƒë·ªì training history sau khi training")
    parser.add_argument("--save-history", action="store_true", default=False,
                        help="L∆∞u training history v√†o file JSON")
    
    return parser.parse_args()


def make_loader(dataset, batch_size, shuffle, num_workers, pin_memory, drop_last=False) -> DataLoader:
    """T·∫°o DataLoader t·ª´ dataset."""
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,
        persistent_workers=num_workers > 0,
    )


def maybe_subset(dataset, limit: Optional[int]):
    """T·∫°o subset c·ªßa dataset n·∫øu c√≥ limit."""
    if limit is None or limit >= len(dataset):
        return dataset
    indices = list(range(limit))
    return Subset(dataset, indices)


def main() -> None:
    """H√†m main ƒë·ªÉ ch·∫°y training."""
    args = parse_args()

    # Set random seed ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # C·∫•u h√¨nh transform cho ·∫£nh
    cfg = TransformConfig(image_size=args.image_size)
    
    # Load datasets
    datasets = build_datasets(
        metadata_path=args.metadata,
        label2id_path=args.label2id,
        cfg=cfg,
        splits=("train", "valid"),
    )

    # L·∫•y s·ªë l·ªõp t·ª´ dataset
    num_classes = len(datasets["train"].label2id) if hasattr(datasets["train"], "label2id") else 4

    # T·∫°o subset n·∫øu c√≥ limit
    train_dataset = maybe_subset(datasets["train"], args.train_limit)
    valid_dataset = maybe_subset(datasets["valid"], args.valid_limit)

    # T·∫°o dataloaders
    train_loader = make_loader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=True,
    )
    valid_loader = make_loader(
        valid_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=False,
    )

    # Ch·ªçn device (GPU n·∫øu c√≥, kh√¥ng th√¨ CPU)
    device = torch.device(args.device) if args.device else torch.device(
        "cuda" if torch.cuda.is_available() else "cpu"
    )

    print("\n" + "="*60)
    print(f"Training Configuration:")
    print("="*60)
    print(f"  Model: {args.model_name}")
    print(f"  Device: {device}")
    print(f"  Image size: {args.image_size}")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Number of classes: {num_classes}")
    print(f"  Training samples: {len(train_dataset)}")
    print(f"  Validation samples: {len(valid_dataset)}")
    print(f"  Pretrained: {args.pretrained}")
    print(f"  Attention heads: {args.heads}")
    print(f"  Dropout: {args.dropout}")
    print("="*60 + "\n")

    # Ki·ªÉm tra s·ªë heads h·ª£p l·ªá
    # V·ªõi ResNet18, c_out=512, c_mid=128
    # c_mid ph·∫£i chia h·∫øt cho c·∫£ heads v√† 4 (y√™u c·∫ßu c·ªßa PositionalEncoding2D)
    # C√°c gi√° tr·ªã heads t·ªët nh·∫•t: 1, 2, 4, 8, 16, 32, 64
    import math
    c_mid_base = 128
    lcm = (args.heads * 4) // math.gcd(args.heads, 4)
    c_mid_actual = (c_mid_base // lcm) * lcm
    if c_mid_actual < lcm:
        c_mid_actual = lcm
    
    # C·∫£nh b√°o n·∫øu c_mid b·ªã thay ƒë·ªïi nhi·ªÅu
    if abs(c_mid_actual - c_mid_base) > 20:
        print(f"‚ö†Ô∏è  Warning: heads={args.heads} s·∫Ω l√†m thay ƒë·ªïi c_mid t·ª´ {c_mid_base} xu·ªëng {c_mid_actual}")
        print(f"   ƒêi·ªÅu n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn performance.")
        print(f"   C√°c gi√° tr·ªã heads khuy·∫øn ngh·ªã: 1, 2, 4, 8, 16, 32, 64")
        print(f"   Continuing anyway...\n")

    # Kh·ªüi t·∫°o model ResNet18_BoTLinear
    model = ResNet18_BoTLinear(
        num_classes=num_classes,
        heads=args.heads,
        pretrained=args.pretrained,
        dropout=args.dropout,
    ).to(device)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Optimizer v·ªõi learning rate kh√°c nhau cho backbone v√† head
    param_groups = get_param_groups(
        model,
        base_lr=args.base_lr,
        head_lr=args.head_lr,
        weight_decay=args.weight_decay,
    )
    optimizer = torch.optim.AdamW(param_groups)

    # Learning rate scheduler
    t_max = args.scheduler_tmax or args.epochs
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, t_max))

    # Gradient scaler cho mixed precision training
    scaler = GradScaler(enabled=device.type == "cuda")

    # Early stopping patience
    patience = args.patience if args.patience > 0 else None

    # B·∫Øt ƒë·∫ßu training
    print("Starting training...\n")
    history, metrics = train_model(
        model_name=args.model_name,
        model=model,
        train_loader=train_loader,
        valid_loader=valid_loader,
        criterion=criterion,
        optimizer=optimizer,
        scaler=scaler,
        scheduler=scheduler,
        gpu_aug=None,
        MEAN=None,
        STD=None,
        epochs=args.epochs,
        patience=patience,
        fps_image_size=args.image_size,
    )

    # In k·∫øt qu·∫£ cu·ªëi c√πng
    print("\n" + "="*60)
    print("Training Finished! Final Metrics:")
    print("="*60)
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key:.<30} {value:.4f}")
        else:
            print(f"  {key:.<30} {value}")
    print("="*60 + "\n")

    # L∆∞u history v√† metrics v√†o file JSON
    if args.save_history:
        # T·∫°o th∆∞ m·ª•c results v·ªõi timestamp
        timestamp = datetime.now().strftime("%d_%m_%Y_%H%M")
        run_dir = Path("results") / f"{args.model_name}_{timestamp}"
        run_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüìÅ ƒêang l∆∞u k·∫øt qu·∫£ v√†o: {run_dir}")
        
        # L∆∞u training history
        history_file = run_dir / "history.json"
        history_serializable = {
            key: [float(v) for v in value] if hasattr(value, '__iter__') else value
            for key, value in history.items()
        }
        with open(history_file, 'w') as f:
            json.dump(history_serializable, f, indent=2)
        print(f"‚úì Training history ƒë√£ l∆∞u: {history_file}")
        
        # L∆∞u final metrics
        metrics_file = run_dir / "metrics.json"
        metrics_serializable = {
            key: float(value) if isinstance(value, (int, float)) else value
            for key, value in metrics.items()
        }
        with open(metrics_file, 'w') as f:
            json.dump(metrics_serializable, f, indent=2)
        print(f"‚úì Final metrics ƒë√£ l∆∞u: {metrics_file}")
        
        # L∆∞u bi·ªÉu ƒë·ªì training
        if MATPLOTLIB_AVAILABLE:
            try:
                import matplotlib.pyplot as plt  # Import l·∫°i ƒë·ªÉ tr√°nh l·ªói
                plot_file = run_dir / "training_plot.png"
                plot_history(
                    history, 
                    title=f"{args.model_name} Training History (Acc: {metrics['valid_acc']:.2%})"
                )
                plt.savefig(plot_file, dpi=300, bbox_inches='tight')
                plt.close()
                print(f"‚úì Bi·ªÉu ƒë·ªì training ƒë√£ l∆∞u: {plot_file}")
            except Exception as e:
                print(f"‚ö† Kh√¥ng th·ªÉ l∆∞u bi·ªÉu ƒë·ªì: {e}")
        else:
            print("‚ö† Matplotlib kh√¥ng kh·∫£ d·ª•ng, b·ªè qua vi·ªác l∆∞u bi·ªÉu ƒë·ªì")
        
        print(f"\n‚úÖ T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {run_dir}\n")

    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì training history (n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu)
    if args.plot:
        print("\nƒêang hi·ªÉn th·ªã bi·ªÉu ƒë·ªì training...")
        try:
            plot_history(
                history, 
                title=f"{args.model_name} Training History (Acc: {metrics['valid_acc']:.2%})"
            )
            if MATPLOTLIB_AVAILABLE:
                import matplotlib.pyplot as plt  # Import l·∫°i ƒë·ªÉ tr√°nh l·ªói
                plt.show()
        except Exception as e:
            print(f"‚ö† Kh√¥ng th·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì: {e}")


if __name__ == "__main__":
    main()
